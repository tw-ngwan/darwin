{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full workspace for training, evaluating, and getting input for model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing and getting a dataset. We use the dataset SEntFiN-v1.1.csv from Kaggle (Source: https://www.kaggle.com/code/ankurzing/sentfin/notebook). \n",
    "\n",
    "ONLY RUN IF YOU HAVE NOT CREATED THE DATASET YET \n",
    "\n",
    "Remember for SGNLP, add the additional argument thingy. Refer to Google Colab \n",
    "Activate conda sgnlp env. Try to so you can test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules \n",
    "import json \n",
    "import pandas as pd \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the files into training and test sets in an 80-20 ratio \n",
    "# (60-20-20 for train, cv, and test more precisely)\n",
    "\n",
    "# Load the data from the csv into a df \n",
    "CSV_FILE = './datasets/finance/SEntFiN-v1.1.csv'\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "del df['S No.']\n",
    "\n",
    "# The full dataframe \n",
    "df['Decisions'] = df['Decisions'].apply(json.loads)\n",
    "csv_data = df.to_dict(orient='list')\n",
    "# print(csv_data)\n",
    "\n",
    "# Change the data into this text format \n",
    "text = []\n",
    "# print(csv_data['Decisions'])\n",
    "get_decision = lambda x: '1' if x == 'positive' else '0' if x == 'neutral' else '-1'\n",
    "for i in range(len(csv_data['Title'])):\n",
    "    for key in csv_data['Decisions'][i]:\n",
    "        text.append('\\n'.join([csv_data['Title'][i].replace(key, '$T$'), key, get_decision(csv_data['Decisions'][i][key])]))\n",
    "        # text.append(csv_data['Title'][i].replace(key, '$T$'))\n",
    "        # text.append(key)\n",
    "        # text.append(get_decision(csv_data['Decisions'][i][key]))\n",
    "\n",
    "\n",
    "# How do I write/append random elements in text to a .raw file, and the remaining elements to another .raw file\n",
    "random.shuffle(text)\n",
    "training_text, test_text = text[:len(text)//5 * 4], text[len(text)//5 * 4:]\n",
    "\n",
    "with open('./datasets/finance/finance_train.raw', 'w') as f:\n",
    "    f.write('\\n'.join(training_text))\n",
    "    \n",
    "with open('./datasets/finance/finance_test.raw', 'w') as f:\n",
    "    f.write('\\n'.join(test_text))\n",
    "\n",
    "# Write the data \n",
    "with open('./datasets/finance/financedata.txt', 'w') as f:\n",
    "    f.write('\\n'.join(text))\n",
    "\n",
    "print(\"All data written. Number of rows: \" + str(len(text) * 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells are for training the model. We are training the Sentic GCN model here on the dataset. \n",
    "\n",
    "First, remember to configure your sentic_gcn_config.json file INSIDE the module itself. The config file is located at path \"config/sentic_gcn_config.json\"\n",
    "\n",
    "We import the modules first, then train the model. \n",
    "\n",
    "IF YOU HAVE ALREADY TRAINED THE MODEL, DO NOT RUN THIS AGAIN! ONLY RUN THIS ONCE! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training modules \n",
    "from sgnlp.models.sentic_gcn.train import SenticGCNTrainer, SenticGCNBertTrainer\n",
    "from sgnlp.models.sentic_gcn.utils import parse_args_and_load_config, set_random_seed\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model. Takes a few hours to complete. Only run if you have not run before. \n",
    "\n",
    "# Required not to throw argparse error \n",
    "sys.argv = ['']\n",
    "del sys \n",
    "\n",
    "# Instantiate the config file and start the training \n",
    "cfg = parse_args_and_load_config(config_path=\"config/sentic_gcn_config.json\")\n",
    "if cfg.seed is not None:\n",
    "    set_random_seed(cfg.seed)\n",
    "# Using SenticGCNTrainer\n",
    "trainer = SenticGCNTrainer(cfg) if cfg.model == \"senticgcn\" else SenticGCNBertTrainer(cfg)\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, evaluate our code to see how accurate your model is. Remember to configure your sentic_gcn_config.json file INSIDE the module itself, under eval_args. Path: \"config/sentic_gcn_config.json\"\n",
    "\n",
    "Acc: 0.7590987868284229 \\\n",
    "F1: 0.7558443499898401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules for evaluation first \n",
    "from sgnlp.models.sentic_gcn.eval import SenticGCNEvaluator, SenticGCNBertEvaluator\n",
    "from sgnlp.models.sentic_gcn.utils import parse_args_and_load_config, set_random_seed\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model's performance on a test dataset \n",
    "\n",
    "# Required not to throw argparse error \n",
    "sys.argv = ['']\n",
    "del sys \n",
    "\n",
    "# Instantiate the config file and start the evaluation \n",
    "cfg = parse_args_and_load_config(config_path=\"config/sentic_gcn_config.json\")\n",
    "print(cfg)\n",
    "if cfg.seed is not None:\n",
    "    set_random_seed(cfg.seed)\n",
    "evaluator = SenticGCNEvaluator(cfg) if cfg.model == \"senticgcn\" else SenticGCNBertEvaluator(cfg)\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating and training the model, we can actually input data into it now and get it to output its evaluation. The code below requests for user input, and provides an evaluated sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tengwei\\anaconda3\\envs\\sgnlp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules first \n",
    "from sgnlp.models.sentic_gcn import(\n",
    "    SenticGCNConfig,\n",
    "    SenticGCNModel,\n",
    "    SenticGCNEmbeddingConfig,\n",
    "    SenticGCNEmbeddingModel,\n",
    "    SenticGCNTokenizer,\n",
    "    SenticGCNPreprocessor,\n",
    "    SenticGCNPostprocessor,\n",
    "    download_tokenizer_files,\n",
    ")\n",
    "from inputhelper import (\n",
    "    inputString, \n",
    "    inputInt, \n",
    "    inputList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets input from the user \n",
    "\n",
    "# constants \n",
    "SENTENCE = \"sentence\"\n",
    "ASPECTS = \"aspects\"\n",
    "\n",
    "# Gets number of inputs \n",
    "num_sentences = inputInt(\"How many sentences do you want to evaluate?\\n\", min=1, max=20)\n",
    "\n",
    "# Instantiate inputs \n",
    "inputs = [{} for _ in range(num_sentences)]\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    inputs[i][SENTENCE] = inputString(\"Enter the text you wish to evaluate:\\n\")\n",
    "    inputs[i][ASPECTS] = inputList(\"Enter the aspects you wish to evaluate, separating them with backslashes (/):\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': ['Financial', 'contagion', 'fears', 'spread', 'in', 'India', 'on', 'Friday', 'as', 'the', 'Adani', \"Group's\", 'crisis', 'worsened,', 'with', 'ratings', 'agency', \"Moody's\", 'warning', 'the', 'conglomerate', 'may', 'struggle', 'to', 'raise', 'capital', 'and', 'S&P', 'cutting', 'the', 'outlook', 'on', 'two', 'of', 'its', 'businesses.', 'Chaotic', 'scenes', 'in', 'both', 'houses', 'of', \"India's\", 'parliament', 'led', 'to', 'their', 'adjournment', 'on', 'Friday', 'as', 'some', 'lawmakers', 'demanded', 'an', 'inquiry', 'after', 'a', 'dramatic', 'meltdown', 'in', 'the', 'stock', 'market', 'values', 'of', 'Indian', 'billionaire', 'Gautam', \"Adani's\", 'companies.', 'The', 'crisis', 'was', 'triggered', 'by', 'a', 'Hindenburg', 'Research', 'report', 'last', 'week', 'in', 'which', 'the', 'U.S.-based', 'short-seller', 'accused', 'the', 'Adani', 'Group', 'of', 'stock', 'manipulation', 'and', 'unsustainable', 'debt.', 'Adani', 'Group,', 'one', 'of', \"India's\", 'top', 'conglomerates,', 'has', 'rejected', 'the', 'criticism', 'and', 'denied', 'wrongdoing', 'in', 'detailed', 'rebuttals,', 'but', 'that', 'has', 'failed', 'to', 'arrest', 'the', 'unabated', 'fall', 'in', 'its', 'shares.', 'In', 'the', 'latest', 'sign', 'of', 'the', 'crisis', 'widening,', \"India's\", 'ministry', 'of', 'corporate', 'affairs', 'has', 'begun', 'a', 'preliminary', 'review', 'of', 'Adani', \"Group's\", 'financial', 'statements', 'and', 'other', 'regulatory', 'submissions', 'made', 'over', 'the', 'years,', 'two', 'government', 'officials', 'told', 'Reuters.'], 'aspects': [[10], [89], [97], [145], [77]], 'labels': [-1, -1, -1, -1, -1]}\n"
     ]
    }
   ],
   "source": [
    "# Gets user input, parses it into inputs for the model, and runs the model \n",
    "\n",
    "# Ensure that input exists \n",
    "try:\n",
    "    assert len(inputs) > 0\n",
    "except NameError:\n",
    "    raise \n",
    "except AssertionError:\n",
    "    raise \n",
    "\n",
    "# Obtaining the tokenizer. NOT SURE what argument to put \n",
    "tokenizer = SenticGCNTokenizer.from_pretrained(\"./tokenizers/senticgcn/\")\n",
    "\n",
    "# Obtaining the config variable for the MODEL\n",
    "config = SenticGCNConfig.from_pretrained(\n",
    "    \"./models/senticgcn/config.json\"\n",
    ")\n",
    "\n",
    "# Obtaining the model itself \n",
    "model = SenticGCNModel.from_pretrained(\n",
    "    \"./models/senticgcn/pytorch_model.bin\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Obtaining the config variable for the embedding model \n",
    "embed_config = SenticGCNEmbeddingConfig.from_pretrained(\n",
    "    \"./embed_models/senticgcn_embed_semeval14_rest/config.json\"\n",
    ")\n",
    "\n",
    "# Obtaining the embedding model itself \n",
    "embed_model = SenticGCNEmbeddingModel.from_pretrained(\n",
    "    \"./embed_models/senticgcn_embed_semeval14_rest/pytorch_model.bin\",\n",
    "    config=embed_config\n",
    ")\n",
    "\n",
    "# Getting the preprocessor from everything \n",
    "preprocessor = SenticGCNPreprocessor(\n",
    "    tokenizer=tokenizer, embedding_model=embed_model,\n",
    "    senticnet=\"./senticNet/senticnet.pickle\",\n",
    "    device=\"cpu\")\n",
    "\n",
    "# Postprocessor for everything \n",
    "postprocessor = SenticGCNPostprocessor()\n",
    "\n",
    "# Getting the raw outputs from the preprocessor \n",
    "processed_inputs, processed_indices = preprocessor(inputs)\n",
    "raw_outputs = model(processed_indices)\n",
    "\n",
    "# Getting the postprocessor outputs \n",
    "post_outputs = postprocessor(processed_inputs=processed_inputs, model_outputs=raw_outputs)\n",
    "\n",
    "for output in post_outputs:\n",
    "    print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Adani': -1, 'Hindenburg': -1}\n"
     ]
    }
   ],
   "source": [
    "# Parse and return the data from the model \n",
    "\n",
    "# Verify that outputs exist \n",
    "# print(post_outputs)\n",
    "try:\n",
    "    assert len(post_outputs) >= 0\n",
    "except NameError:\n",
    "    raise\n",
    "except AssertionError:\n",
    "    raise \n",
    "\n",
    "# List storing all dictionaries with sentiments \n",
    "total_results = []\n",
    "\n",
    "for output in post_outputs:\n",
    "    output_sentiments = {} \n",
    "    for i in range(len(output['labels'])):\n",
    "        resultant_phrase = ' '.join([output['sentence'][j] for j in output['aspects'][i]])\n",
    "        # resultant_phrase = ' '.join([output['sentence'][j] for j in range(len(output['aspects'][i]))])\n",
    "        output_sentiments[resultant_phrase] = output['labels'][i]\n",
    "\n",
    "# Better way of expressing, please...\n",
    "print(output_sentiments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c099a42255214e7aa5ab3549b724f6e6c75158701d3aabfc4194051e96159df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
